<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"/><meta name="og:site_name" content="Grant Emerson"/><link rel="canonical" href="http://www.grantemerson.co/blog/BuildingASynthesizerInSwift"/><meta name="twitter:url" content="http://www.grantemerson.co/blog/BuildingASynthesizerInSwift"/><meta name="og:url" content="http://www.grantemerson.co/blog/BuildingASynthesizerInSwift"/><title>Building a Synthesizer in Swift | Grant Emerson</title><meta name="twitter:title" content="Building a Synthesizer in Swift | Grant Emerson"/><meta name="og:title" content="Building a Synthesizer in Swift | Grant Emerson"/><meta name="description" content="Making audio waveforms with AVAudioEngine"/><meta name="twitter:description" content="Making audio waveforms with AVAudioEngine"/><meta name="og:description" content="Making audio waveforms with AVAudioEngine"/><meta name="twitter:card" content="summary_large_image"/><link rel="stylesheet" href="/styles.css" type="text/css"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="shortcut icon" href="/favicon.ico" type="icon"/><link rel="alternate" href="/feed.rss" type="application/rss+xml" title="Subscribe to Grant Emerson"/><meta name="twitter:image" content="http://www.grantemerson.co/ArticleImages/BuildingASynthesizerInSwift.jpeg"/><meta name="og:image" content="http://www.grantemerson.co/ArticleImages/BuildingASynthesizerInSwift.jpeg"/></head><body class="item-page"><header><div class="wrapper"><a class="site-name" href="/">Grant Emerson</a><nav><ul><li><a href="/about">About Me</a></li><li><a class="selected" href="/blog">Blog</a></li><li><a href="/projects">Projects</a></li><li><a href="mailto:grantjemerson@gmail.com">Contact</a></li></ul></nav></div></header><div class="wrapper"><article><div class="content"><h1>Building a Synthesizer in Swift</h1><span class="article-date">Published on Jul 25, 2019</span><div>
    <img class="article-image" src="/ArticleImages/BuildingASynthesizerInSwift.jpeg">
    <p class="article-image-description">A random synthesizer...</p>
</div><p>During WWDC ’19, Apple quietly announced some updates to the AVAudioEngine with a short video.</p><p>Included in these updates were two brand new AVAudioNodes called AVAudioSinkNode and AVAudioSourceNode. In this piece, we’ll be focusing on how the AVAudioSourceNode can be used to build a musical synthesizer for iOS.</p><p>In Apple’s slideshow they collectively defined the two new nodes as a wrapper for “<em>a user-defined block that allows apps to send or receive audio from AVAudioEngine.</em>” In our case, we’ll be sending audio data to the output of the audio signal processing network. The AVAudioSourceNode provides a trailing closure expression which takes in four parameters. We will only need the last two, which are of type AVAudioFrameCount and UnsafeMutablePointer (which points to a list of audio buffers). The closure’s return type is OSStatus, which will be used to indicate whether or not the DSP (Digital Signal Processing) code for our synthesizer’s oscillator is running smoothly.</p><p>The AVAudioSourceNode can be used in realtime and manual rendering mode, which means that it can be used to write audio straight to an audio file or, in our case, create sounds in a live context.</p><p>Lastly, Apple gives us an extremely important warning: the code you write in the AVAudioSourceNode’s block <em>must be realtime compliant</em>. This means that while on the audio thread, no objects should be initialized and no memory should be allocated. The realtime aspect of audio programming is especially important because lag in audio code can result in buffer under or overflows, which create clicks and pops. These clicks are devastating in a live performance as not only could they damage a user’s hearing, they could also damage the user’s speakers.</p><p>Now, with the introduction out of the way, let’s get building our synthesizer!</p><p>Here's a quick video of the app we will be building:</p><iframe width=420 height=236 style="padding-top:10px; padding-bottom:10px; display: block; margin-left: auto; margin-right: auto;"src="https://www.youtube.com/embed/1ouhwoUzhGU" frameborder="0" allowfullscreen></iframe><p>To follow along with this tutorial you’ll need Xcode 11 or later. If you want to run your finished application on a device instead of a simulator, that device needs to have iOS 13 or later installed.</p><p>Once your Xcode is up to date, open it and navigate to File -&gt; New -&gt; Project. Select iOS and Single View Application. Set the Product Name to “Swift Synth” or whatever name you like. Click Next, navigate to the directory you want to save your application in, and click Create.</p><p>Let’s take care of some maintenance. For this tutorial, I’ll be laying out the UI programmatically. Feel free to use storyboards if you like — it should be easy to follow along either way.</p><p>The project should have opened up to the project settings. If you don’t want to use Storyboards, scroll down to the Deployment Info tab in General and clear the text field to the right of Main Interface. Next, locate the Main.storyboard file in the Project Navigator and delete it. Finally, we need to enter the Info.plist file and delete the field which pertains to the Storyboard Name.</p><div>
    <img class="article-image" src="/ArticleImages/BuildingASynthesizerInSwiftBlogTutorialImage1.jpeg">
    <p class="article-image-description">Follow these steps to clear the Main Interface.</p>
</div><div>
    <img class="article-image" src="/ArticleImages/BuildingASynthesizerInSwiftBlogTutorialImage2.jpeg">
    <p class="article-image-description">Click the minus button where the arrow is pointing to remove the indicated field. You will have to drill down through the structure to reach the specified field.</p>
</div><p>Click the minus button where the arrow is pointing to remove the indicated field. You will have to drill down through the structure to reach the specified field.</p><p>One of the updates in iOS 13 and Xcode 11 was the introduction of the SceneDelegate.swift. This manages various <code>UIScene</code>s in your app and specifically interacts with the top-level <code>UIWindowScene</code> to manage multiple windows.</p><p>To set the root view controller at configuration time, we now use the <code>scene willConnectTo session: UISceneSession</code> function. First, we should attempt to cast the scene to a <code>UIWindowScene</code>. If that succeeds, we can continue by initializing the <code>window</code> property of the <code>SceneDelegate</code>.</p><p>We will pass the <code>bounds</code> of windowScene’s <code>coordinateSpace</code> into the initializer which includes one frame. Next, set the window’s <code>windowScene</code> property to the <code>windowScene</code> constant we created earlier. Set the <code>rootViewController</code> of the window to <code>ViewController</code>.</p><p>Lastly, make sure to call the <code>makeKeyAndVisible</code> method on window to present the UI. To ensure everything is working, build and run the application. You should see a blank screen and no errors in the console.</p><p>Now, open ViewController.swift. For the sake of clarity, command click the class name and choose the rename option from the dropdown. Type SynthViewController in the text box to rename the file and class. The first thing I like to do when programming a new view controller is to stub it out with mark comments:</p><p>It’ll be easier to create the audio part of the app first. Create a new Swift file (File -&gt; New -&gt; File) and call it Synth. Start by importing <code>AVFoundation</code> and <code>Foundation</code>:</p><pre><code><span class="keyword">import</span> AVFoundation
<span class="keyword">import</span> Foundation
</code></pre><p>Then create a class called Synth and stub it out with the same mark comments. You’ll also need to create an initializer, although we’ll be modifying its parameters later:</p><p>We’ll be making Synth a <code>singleton</code> by adding a static shared instance property to its definition. This will allow us to access it from any view controller with ease:</p><p>Now we’ll add a few properties pertaining to the actual audio of our synthesizer. The first will be a <code>volume</code> property which will allow us to simulate turning off and on the synthesizer. The most important variable in our synthesizer is the engine, which is an AVAudioEngine. The AVAudioEngine will host the sound making AVAudioNodes that we add to our signal chain. The last three are timing variables, which we will discuss in more detail later.</p><p>To initialize these properties, head to the initializer and start by initializing the <code>audioEngine</code>. Next, create two constants that reference the <code>mainMixerNode</code> and <code>outputNode</code> of the <code>audioEngine</code>.</p><p>The <code>mainMixerNode</code> is a singleton that is connected to the <code>outputNode</code> on the first mention. It acts as an intermediary between the source nodes and the <code>outputNode</code>.</p><p>Then create a <code>format</code> constant by calling the <code>inputFormat</code> function for bus 0 on the <code>outputNode</code>. The format will provide us with the default audio settings for the device we are working with. For instance, we can set our <code>sampleRate</code> property by accessing the format’s <code>sampleRate</code> property. If the concept of sample rate is new to you, I suggest you take a quick detour and check out The Audio Programmer’s <a href="https://www.youtube.com/watch?v=PLuBamvtBZU">video</a> about the fundamentals of audio software on youtube.</p><p>Next, set the <code>deltaTime</code> float to one over the <code>sampleRate</code>. Delta time is duration each sample is held for. For instance, if the <code>sampleRate</code> was 44,100 Hz, you would take one second and divide it into 44,100 to represent each of the samples.</p><p>We’ve now reached the point where we can start diving into <code>AVAudioSourceNode</code>.</p><p>Start by defining a <code>typealias</code> called <code>Signal</code> outside the <code>Synth</code> class definition. <code>Signal</code> will be a closure type which takes in one float to represent time and returns one float for the audio sample:</p><pre><code><span class="keyword">typealias</span> Signal = (<span class="type">Float</span>) -&gt; (<span class="type">Float</span>)
</code></pre><p>Now, inside <code>Synth</code> add a variable called <code>signal</code> of type <code>Signal</code>.</p><p>Next, add a <code>sourceNode</code> variable of type <code>AVAudioSourceNode</code>. Make sure to <code>lazily</code> initialize <code>sourceNode</code> as we will be referencing <code>self</code> within its trailing closure. You can hit enter to auto-complete a basic closure structure. For the two parameters, type an underscore as they are not necessary.</p><p>The last two should be named <code>frameCount</code> and <code>audioBufferList</code>. Within the block, start by defining a pointer called <code>ablPointer</code> as a <code>UnsafeMutableAudioBufferListPointer</code> with <code>audioBufferList</code> in the initializer. The <code>audioBufferList</code> holds an array of audio buffer structures that we will fill with our custom waveforms. Buffers are used in audio to give applications more than 1/44,100 of a second to generate samples within the render block. Audio buffers generally contain between 128 and 1024 samples.</p><p>Next, we create a for-loop to iterate through index values between 0 and our <code>frameCount</code> variable.</p><p>In audio, frames are sets of samples that occurred at the same time. In stereo audio, each frame contains two samples–one for the left ear and another for the right ear. In our case, we’ll be setting both samples to the same value because our synth is monophonic.</p><p>Inside of the for-loop, we will obtain the <code>sampleVal</code> by calling our signal closure with Synth’s time property, then advance time with <code>deltaTime</code>.</p><p><code>ABLPointer</code> points to an array, which means we can treat it as such and iterate through its contents within a nested for-loop.</p><p>For each <code>buffer</code> element, we must cast it to a float pointer. We can then index the buffer at the current frame and set it the <code>sampleVal</code> found earlier.</p><p>Lastly, don’t forget to return <code>noErr</code> if everything succeeds.</p><p>We can now return to the initializer and finish setting up Synth.</p><p>Start by adding a parameter to the initializer called <code>signal</code> of type escaping <code>Signal</code>. Then, inside the initializer, set <code>self.signal</code> to the <code>signal</code> argument.</p><p>Create an <code>inputFormat</code> of type <code>AVAudioFormat</code> — filling in the arguments with format’s properties and limiting the channels to one. Then, attach <code>sourceNode</code> to the <code>audioEngine</code> to introduce it the audio graph.</p><p>We can now connect the <code>sourceNode</code> to the <code>mainMixer</code> using <code>inputFormat</code>.</p><p>The last step is to connect the <code>mainMixer</code> to the <code>outputNode</code>. You can set <code>mainMixer</code>’s <code>outputVolume</code> to a starting value of 0 as it shouldn’t make sound without a user first requesting to do so.</p><p>Finally, we need to call the <code>start</code> method on the <code>audioEngine</code> to initialize the hardware I/O nodes. This function can throw an error so you need to proceed the line with the try keyword and wrap the entire statement in a do-catch block.</p><p>Since we have encapsulated all the properties of Synth fairly well, we will need one public accessor method to set the signal of Synth.</p><pre><code><span class="comment">// MARK: Public Functions</span>
<span class="keyword">public func</span> setWaveformTo(<span class="keyword">_</span> signal: <span class="keyword">@escaping</span> <span class="type">Signal</span>) { 
    <span class="keyword">self</span>.<span class="property">signal</span> = signal
}
</code></pre><p>To start producing audio, we need to make a set of closure expressions which conform to the <code>Signal</code> type. Let’s start by creating a swift file called <code>Oscillator</code>. Feel free to move the typealias for <code>Signal</code> over to this file as it should fit in nicely.</p><p>Start by creating a struct called <code>Oscillator</code>. Add two static Float variables to <code>Oscillator</code> called amplitude and frequency, giving them initial values of 1 and 440 (Concert A) respectively.</p><pre><code><span class="keyword">import</span> Foundation

<span class="keyword">struct</span> Oscillator { 
    <span class="keyword">static var</span> amplitude: <span class="type">Float</span> = <span class="number">1</span> 
    <span class="keyword">static var</span> frequency: <span class="type">Float</span> = <span class="number">440</span>
}
</code></pre><p>Let’s start by building the sine oscillator, as it is the simplest.</p><p>Inside <code>Oscillator</code>, create a constant static closure expression called <code>sine</code> with a Float parameter for time and a return type of Float for the samples it will output. Within the closure block, calculate the sine of 2 * pi * <code>Oscillator.frequency</code> * <code>time</code>. Then multiply the output by <code>Oscillator.amplitude</code> and return the result.</p><p>If you took trigonometry you’ll know that sine is a periodic function of time with a period equal to (2 * pi) over b, where b is the factor that time or x is being multiplied by before being passed into the function. In our case, b is equal to (2 * pi * <code>Oscillator.frequency</code>). That means the period of our sine wave is (1 / <code>Oscillator.frequency</code>). This makes perfect sense because our frequency is in Hz or cycles per second. If there are 440 cycles per second in a sine wave, each cycle is allotted 1 / 440th of a second.</p><p>The next oscillator we’ll build is the triangle wave. This will also be a constant static closure expression, with the same parameters.</p><p>For a triangle wave, we separate the wave into three parts: the initial incline, the turning point, and the latter incline. We first calculate the period of the triangle wave by dividing one by <code>Oscillator.frequency</code>. However, because there is no triangle function built into the standard library, we have to calculate where the current sample is located relative to the current cycle.</p><p>We can find the <code>currentTime</code> constant by taking the floating point remainder of the total <code>time</code> elapsed divided by the <code>period</code>. For example, if <code>time</code> currently equals 17.5 and the <code>period</code> is 5, the remainder of 17.5 / 5 will be 2.5.</p><p>With this information, we can create a <code>value</code> constant that holds the progress percentage of the current cycle. This percentage is found by dividing the <code>currentTime</code> (2.5) by the <code>period</code> (5). This shows us that we are exactly 50% of the way through the current waveform.</p><p>We can use the <code>value</code> constant to calculate the <code>result</code> sample value. If <code>value</code> is less than 0.25, we are in the first fourth of the triangle and are inclining from 0 to 1. For that reason, we set <code>result</code> equal to 4 times the current <code>value</code>. If <code>value</code> is greater than or equal to 0.25 and less than 0.75, the waveform dips from 1 to -1. <code>Result</code> will equal value multiplied by 4 and subtracted from two. We know from the last if-statement that (value * 4) ended at 1, therefore we will be starting at (2–1=1) which is the peak. The last else-statement works in a similar way to the previous two. The last step is to convert <code>result</code> back into a <code>Float</code> and multiply <code>Oscillator.amplitude</code>.</p><p>The next two oscillators we will build are the sawtooth and square. Following the same convention as before, these will both be static closure expressions.</p><p>We will also reuse the same mathematics applied in the triangle wave to find the location of each sample in the sawtooth and square waves. In the sawtooth oscillator, the percentage value will simply be multiplied by 2 (0–2). The final <code>result</code> will be that value minus 1 (-1–1) and multiplied by <code>Oscillator.amplitude</code>.</p><p>In the square wave, if value is less than 0.5 the closure returns the negative of <code>Oscillator.amplitude</code>. Otherwise, it simply returns <code>Oscillator.amplitude</code>. This limitation of only 2 states creates abrupt, high energy transitions which adds a series of upper odd harmonics.</p><p>Last but not least, we will create a <code>whiteNoise</code> oscillator. This is by far the easiest to program and wrap one’s mind around, as it is simply random <code>Float</code> sample values.</p><p>With the release of Swift 4.2, primitive numerical types such as <code>Float</code> got a new static method called random(in:). With this new feature we can simply pass in a closed range (…) from -1 to 1 and multiply it by <code>Oscillator.amplitude</code> to obtain the sample.</p><p>Awesome, we did it!</p><p>We now have a 5-waveform oscillator built entirely with Swift. Let’s head back to <code>SynthViewController</code> so that we can give it a proper user interface.</p><p>For starters, we’ll need a <code>UISegmentedControl</code> to switch between our waveforms. The square icons I used can be found <a href="https://drive.google.com/drive/folders/1Po35zw-zJwqVwSuzSNpqbc4xBha_19wv?usp=sharing">here</a>. I did not create these icons, so please don’t use them commercially without first purchasing them from <a href="https://thenounproject.com/marcogaltarossa/">this artists page</a> on TheNounProject.</p><p>The other component we will add is UILabel to display the frequency and amplitude of the current waveform being produced. We’ll initialize these private variables lazily so that we can access self within the closure expression as seen below:</p><p>Then we can create two new private functions called <code>setUpView</code> and <code>setUpSubviews</code>. They’ll both be called at the start of our app’s lifecycle in the <code>viewDidLoad</code> function.</p><p>Let’s implement the <code>updateOscillatorWaveform</code> function that we added as a selector to our <code>waveformSelectorSegmentedControl</code>.</p><p>First, to interface between the <code>UISegmentedControl</code>’s index values and the oscillator type, create an enum of type <code>Int</code> called <code>Waveform</code> somewhere within your Oscillator.swift file.</p><p>Then add the following cases: <code>sine</code>, <code>triangle</code>, <code>sawtooth</code>, <code>square</code>, and <code>whiteNoise</code>. They should be in <em>exactly that order,</em> as long as you copied the <code>images</code> array exactly the way I wrote it within the declaration of <code>waveformSelectorSegmentedControl</code>.</p><pre><code><span class="keyword">enum</span> Waveform: <span class="type">Int</span> { <span class="keyword">case</span> sine, triangle, sawtooth, square, whiteNoise }
<span class="keyword">struct</span> Oscillator {  
...
</code></pre><p>The actual implementation of <code>updateOscillatorWaveform</code> will involve calling the <code>rawValue</code> initializer on <code>Waveform</code> with <code>waveformSelectorSegmentedControl</code>’s <code>selectedSegmentIndex</code> property.</p><p>Then, a switch statement can be defined with the resulting <code>waveform</code>. For each of the five cases, call <code>Synth.shared.setWaveformTo</code> with the respective <code>Oscillator</code> waveform.</p><p>We should also quickly implement a <code>setPlaybackStateTo(state:)</code> function to turn our synth on and off. This function will simply take in a boolean called <code>state</code> and set <code>Synth.shared.volume</code> to 0.5 or 0 using a ternary operator.</p><p>Finally, we use the touch related methods already included in <code>UIViewController</code> to allow the user to manipulate the oscillator’s pitch.</p><p>That’s it! Build it and run the app on an iOS 13 device or simulator to test it.</p><p>I hope you had a great time learning how to build synths in Swift. If you have any bugs or if I made any mistakes, please feel free to leave a comment below.</p><p>If you want to download the final Xcode project, you can find it on Github <a href="https://github.com/GrantJEmerson/SwiftSynth">here</a>. Also, a lot of the code I used in this tutorial was found in Apple’s sample project <a href="https://developer.apple.com/documentation/avfoundation/audio_track_engineering/building_a_signal_generator">here</a>. If you want to learn more about what’s new in AVAudioEngine, you can find the WWDC video <a href="https://developer.apple.com/videos/play/wwdc2019/510/">here</a>.</p><p>Until next time!</p><span>Tagged with: </span><ul class="tag-list"><li><a href="/tags/avfoundation">AVFoundation</a></li><li><a href="/tags/swift">Swift</a></li></ul></div></article></div><footer><p>Built using <a href="https://github.com/apple/swift">Swift</a> and <a href="https://github.com/johnsundell/publish">Publish</a>.</p><p>© 2023 Grant Emerson</p><p><a href="/feed.rss">RSS</a> | <a href="mailto:grantjemerson@gmail.com">Email</a></p></footer></body></html>